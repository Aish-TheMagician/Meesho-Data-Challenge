# -*- coding: utf-8 -*-
"""Vision Transformer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OJLILxu2-NucRZCZbEet29PevhJfhfUq
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

df1 = pd.read_csv("/kaggle/input/visual-taxonomy/train.csv")
df2 = pd.read_csv("/kaggle/input/visual-taxonomy/test.csv")
sample = pd.read_csv('/kaggle/input/visual-taxonomy/sample_submission.csv')
df1["id"] = df1["id"].astype(str).str.zfill(6)
df2["id"] = df2["id"].astype(str).str.zfill(6)

df1['image_path'] = df1['id'].apply(lambda x: f"/kaggle/input/visual-taxonomy/train_images/{str(x).zfill(6)}.jpg")

# Calculate the number of rows to remove based on the condition
rem_row = df1.index[df1.apply(lambda row: row.isna().sum() > 13 - row['len'], axis=1)].tolist()

# rem_row = []
# for i in range(df1.shape[0]):
#     if df1.iloc[i,:].isna().sum() > 11 - df1.loc[i,'len']:
#         rem_row.append(i)

df1.drop(rem_row, axis = 0, inplace = True)
df1.reset_index(drop= True, inplace = True)

# for i in range(df1.shape[0]):
#     if df1.iloc[i,:].isna().sum() == 10 - df1.loc[i,'len']:
#         df1.iloc[i,:] = df1.iloc[i,:].fillna("extra")

df1

# #new one

# import pandas as pd
# from sklearn.compose import ColumnTransformer
# from sklearn.impute import SimpleImputer

# # Function to apply SimpleImputer on the useful columns for a specific category
# def apply_column_transformer(df_temp):
#     # Get the maximum number of useful attributes for this category based on the 'len' column
#     max_len = df_temp['len'].max()

#     # Create a list of useful columns (e.g., 'attr_1', 'attr_2', ..., 'attr_max_len')
#     useful_columns = [f'attr_{i}' for i in range(1, max_len + 1)]

#     # Create a ColumnTransformer to apply SimpleImputer to only the useful columns
#     column_transformer = ColumnTransformer(
#         transformers=[
#             ('imputer', SimpleImputer(strategy='most_frequent'), useful_columns)  # Impute only the useful columns
#         ],
#         remainder='passthrough'  # Leave other columns unchanged
#     )

#     # Apply the transformer (fit and transform)
#     transformed_array = column_transformer.fit_transform(df_temp)

#     # Now manually combine the transformed useful columns and the non-transformed columns
#     df_temp_imputed = pd.DataFrame(transformed_array, columns=useful_columns + [col for col in df_temp.columns if col not in useful_columns])

#     # Reset index of the imputed DataFrame to match original DataFrame
#     df_temp_imputed.index = df_temp.index  # Ensure the indices are the same

#     # Ensure data types are preserved for columns that exist in both DataFrames
#     for col in df_temp.columns:
#         if col in df_temp_imputed.columns:  # Check if the column exists in the transformed DataFrame
#             df_temp_imputed[col] = df_temp_imputed[col].astype(df_temp[col].dtype)

#     return df_temp_imputed

# df_imputed = df1.copy()

# # Get all unique categories in the dataset
# categories = df_imputed['Category'].unique()

# # Loop over each category and apply the column transformer function
# for category in categories:
#     # Step 1: Filter the DataFrame for the current category
#     df_temp = df_imputed[df_imputed['Category'] == category].copy()

#     # Step 2: Apply the column transformer to this category's DataFrame
#     df_temp_imputed = apply_column_transformer(df_temp)

#     # Step 3: Update the original DataFrame with the imputed values
#     # Determine the useful columns for this category
#     useful_columns = [f'attr_{i}' for i in range(1, df_temp['len'].max() + 1)]

#     # Assign the imputed values back to the original DataFrame
#     df_imputed.loc[df_temp.index, useful_columns] = df_temp_imputed[useful_columns]

import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.preprocessing import OrdinalEncoder

def apply_knn_imputer_categorical(df_temp, n_neighbors=25):

    max_len = df_temp['len'].max()
    useful_columns = [f'attr_{i}' for i in range(1, max_len + 1)]

    encoder = OrdinalEncoder()
    df_encoded = df_temp.copy()
#     print(df_temp[useful_columns])
    df_encoded[useful_columns] = encoder.fit_transform(df_temp[useful_columns])
    imputer = KNNImputer(n_neighbors=n_neighbors)
    df_imputed_array = imputer.fit_transform(df_encoded[useful_columns])

    df_imputed = pd.DataFrame(df_imputed_array, columns=useful_columns, index=df_temp.index)
#     print(df_imputed)

    df_imputed[useful_columns] = df_imputed[useful_columns].round().astype(int)
#     print(df_imputed)

    df_imputed[useful_columns] = encoder.inverse_transform(df_imputed[useful_columns])
#     print(df_imputed)
    cols = [x for x in df_temp.columns.tolist() if x not in useful_columns]
    df_imputed = pd.concat([df_imputed, df_temp[cols]], axis = 1)
    df_imputed = df_imputed[df_temp.columns]
#     print(df_imputed)

    return df_imputed

# Create a copy of the original DataFrame to work on
df_imputed = df1.copy()

# Get all unique categories in the dataset
categories = df_imputed['Category'].unique()

# Loop over each category and apply the column transformer function
for category in categories:
    # Step 1: Filter the DataFrame for the current category
    df_temp = df_imputed[df_imputed['Category'] == category].copy()

    # Step 2: Apply the column transformer to this category's DataFrame
    df_temp_imputed = apply_knn_imputer_categorical(df_temp)

    # Step 3: Update the original DataFrame with the imputed values
    # Determine the useful columns for this category
    useful_columns = [f'attr_{i}' for i in range(1, df_temp['len'].max() + 1)]

    # Assign the imputed values back to the original DataFrame
    df_imputed.loc[df_temp.index, useful_columns] = df_temp_imputed[useful_columns]

# Now, df_imputed contains the imputed values for all categories

df_imputed

from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Dictionary to hold encoders and data for each category
encoders = {}
category_dfs_encoded = {}

# Function to one-hot encode the attributes and store the encoder
def one_hot_encode_category(df, category, num_attributes):
    # Initialize OneHotEncoder
    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

    # Select the useful columns (attributes)
    useful_columns = [f'attr_{i}' for i in range(1, num_attributes + 1)]

    # Fit and transform the attributes
    ohe_encoded = ohe.fit_transform(df[useful_columns])

    # Store the encoder for later decoding
    encoders[category] = ohe

    # Combine the encoded attributes with id, Category, and image_path
    one_hot_df = pd.concat([df[['id', 'Category', 'image_path']].reset_index(drop=True),
                            pd.DataFrame(ohe_encoded, columns=ohe.get_feature_names_out())], axis=1)

    return one_hot_df

# Split the data by category and apply one-hot encoding
category_attributes = {
    'Men Tshirts': 5,
    'Sarees': 10,
    'Kurtis': 9,
    'Women Tshirts': 8,
    'Women Tops & Tunics': 10
}

for category, num_attributes in category_attributes.items():
    # Filter the DataFrame by category
    df_temp = df_imputed[df_imputed['Category'] == category].copy()

    # Apply one-hot encoding
    df_encoded = one_hot_encode_category(df_temp, category, num_attributes)

    # Store the one-hot encoded DataFrame in a dictionary
    category_dfs_encoded[category] = df_encoded

# Example: Access the one-hot encoded DataFrames for each category
df_men_tshirts_encoded = category_dfs_encoded['Men Tshirts']
df_sarees_encoded = category_dfs_encoded['Sarees']
df_kurtis_encoded = category_dfs_encoded['Kurtis']
df_women_tshirts_encoded = category_dfs_encoded['Women Tshirts']
df_women_tunics_encoded = category_dfs_encoded['Women Tops & Tunics']

# Print the shape of the one-hot encoded DataFrame for one category
print(f"Shape of Men Tshirts encoded data: {df_men_tshirts_encoded.shape}")
print(f"Shape of Sarees encoded data: {df_sarees_encoded.shape}")
print(f"Shape of Kurtis encoded data: {df_kurtis_encoded.shape}")
print(f"Shape of Women Tshirts encoded data: {df_women_tshirts_encoded.shape}")
print(f"Shape of Women Tunics encoded data: {df_women_tunics_encoded.shape}")

# Display the first few rows of the encoded data for "Men Tshirts"
print(df_kurtis_encoded.head())

category_dfs_encoded['Men Tshirts']

df2['image_path'] = df2['id'].apply(lambda x: f"/kaggle/input/visual-taxonomy/test_images/{str(x).zfill(6)}.jpg")
df2

test_df = df2.copy()

import torch
from transformers import ViTForImageClassification, ViTFeatureExtractor
from torch.utils.data import DataLoader
from PIL import Image
import pandas as pd
import numpy as np
import os
import torch.optim as optim
import torch.nn as nn

# Load the feature extractor
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# Set device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Custom dataset class for ViT
class ProductDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        image_path = row['image_path']
        image = Image.open(image_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        labels = row.iloc[3:].values.astype(float)  # Starting from 4th column onwards (attributes)
        return image, torch.tensor(labels, dtype=torch.float32)

# Apply ViT feature extractor transformations to the dataset
def vit_transform(image):
    return feature_extractor(images=image, return_tensors="pt").pixel_values[0]

# Prepare dataset
def prepare_dataset(df):
    return ProductDataset(df, transform=vit_transform)

# Initialize the ViT model for multi-label classification
def create_vit_model(num_labels):
    model = ViTForImageClassification.from_pretrained(
        'google/vit-base-patch16-224-in21k',
        num_labels=num_labels
    )
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.3),
        nn.Linear(model.config.hidden_size, num_labels)
    )
    return model

def train_vit_model(model, dataloader, num_epochs=1):
    model = model.to(device)
    criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)

    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images).logits  # Forward pass
            loss = criterion(outputs, labels)
            loss.backward()  # Backpropagation
            optimizer.step()  # Update weights
            running_loss += loss.item()
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}")

    return model

# Generalized function for creating and training the ViT model for any category
def create_and_train_vit_model_for_category(df_encoded, category_name, num_epochs=3):
    # Calculate the number of labels based on the encoded dataframe
    num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'

    # Create the ViT model
    vit_model = create_vit_model(num_labels)

    # Prepare dataset and DataLoader
    train_dataset = prepare_dataset(df_encoded)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # Train the model
    trained_model = train_vit_model(vit_model, train_loader, num_epochs)

    # Save the trained model
    model_filename = f'vit_model_{category_name}.pth'
    torch.save(trained_model.state_dict(), model_filename)
    print(f"Model for {category_name} saved as {model_filename}")

    return trained_model

# Function to load and test the model for a given category
def test_vit_model(df_test, category_name, num_labels):
    # Prepare test dataset
    test_dataset = prepare_dataset(df_test)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Load the trained model
    model = create_vit_model(num_labels)
    model.load_state_dict(torch.load(f'vit_model_{category_name}.pth'))
    model = model.to(device)
    model.eval()

    # Make predictions
    predictions = []
    with torch.no_grad():
        for images, ids in test_loader:
            images = images.to(device)
            outputs = model(images).logits
            preds = torch.sigmoid(outputs)
            predictions.append(preds.cpu().numpy())

    return np.concatenate(predictions, axis=0)

# Decode predictions using the OneHotEncoder dynamically based on the category and number of attributes
def decode_predictions(predictions, encoder):
    decoded_predictions = []

    for pred in predictions:
        decoded = []
        val = 0
        for i in range(len(encoder.categories_)):
            # Get the probabilities for the current attribute (i-th group of one-hot encoded columns)
            attr_probs = pred[val : val + len(encoder.categories_[i])]
            val = val + len(encoder.categories_[i])

            # Ensure that we pick the category with the highest probability, even if all are below the threshold
            max_index = np.argmax(attr_probs)  # Find the index of the highest probability

            decoded.append(encoder.categories_[i][max_index])  # Use the encoder to map back the category

        decoded_predictions.append(decoded)

    return np.array(decoded_predictions)

def prepare_submission(df_test, decoded_attributes, category_name, num_attributes):
    # Prepare the submission data
    submission_data = {
        'id': df_test['id'].values  # Include the 'id' column
    }

    # Get the actual number of decoded attributes from the shape of decoded_attributes
    actual_num_attributes = decoded_attributes.shape[1]

    # Dynamically add attribute columns to the dictionary based on the actual number of decoded attributes
    for i in range(1, actual_num_attributes + 1):
        submission_data[f'attr_{i}'] = decoded_attributes[:, i - 1]  # Adjust for zero-indexing in arrays

    # Convert the dictionary to a DataFrame
    submission_df = pd.DataFrame(submission_data)

    # Save the submission DataFrame to a CSV file
    submission_filename = f"{category_name}_predictions.csv"
    submission_df.to_csv(submission_filename, index=False)
    print(f"Submission file saved: {submission_filename}")


# Generalized function to handle decoding and submission for any category
def decode_and_save_submission(predictions, df_test, encoder, category_name, num_attributes):
    # Convert the predictions array
    pred_array = predictions

    # Decode the predictions using the stored OneHotEncoder
    decoded_attributes = decode_predictions(pred_array, encoder)

    # Prepare the submission DataFrame
    submission_df = prepare_submission(df_test, decoded_attributes, num_attributes)

    # Save the submission DataFrame to CSV
    submission_file = f'{category_name}_predictions.csv'
    submission_df.to_csv(submission_file, index=False)

    print(f"Submission file for {category_name} saved as {submission_file}")

    return submission_df

# Function to save the trained model for a specific category
def save_model(trained_model, category_name):
    model_filename = f'vit_model_{category_name}.pth'
    torch.save(trained_model.state_dict(), model_filename)
    print(f"Model for {category_name} saved as {model_filename}")

categories = ['Sarees', 'Kurtis', 'Women Tshirts', 'Women Tops & Tunics', 'Men Tshirts']

def pipeline(categories, category_dfs_encoded, test_df , num_epoch = 3):
    # Loop over each category provided in the input
    for category in categories:
        print(f"\nProcessing category: {category}")

        # 1. Prepare the dataset and dataloader for the category
        df_encoded = category_dfs_encoded[category]  # Get the encoded dataframe for the category

        # 2. Create and train the model for this category
        num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'
        trained_model = create_and_train_vit_model_for_category(df_encoded ,category,num_epoch)

        # 3. Save the trained model
        save_model(trained_model, category)

        # 4. Prepare the test data for this category
        df_test_category = test_df[test_df['Category'] == category].copy()
        df_test_category['image_path'] = df_test_category['id'].apply(lambda x: f"/kaggle/input/visual-taxonomy/test_images/{x}.jpg")
        test_dataset = prepare_dataset(df_test_category)  # Prepare the test dataset
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        # 5. Test the model on the test data
        predictions = test_vit_model(df_test_category, category, num_labels)  # Get predictions
        print(pd.DataFrame(predictions))

        # 6. Decode the predictions using the stored OneHotEncoder
        pred_array = np.concatenate(predictions, axis=0)  # Concatenate along rows
        pred_array = pred_array.reshape(-1, num_labels)  # Reshape to (samples, num_labels)

        decoded_predictions = decode_predictions(pred_array, encoders[category])
        print(pd.DataFrame(decoded_predictions))


        # 7. Prepare and save the submission for this category
        prepare_submission(df_test_category, decoded_predictions, category, num_labels)

pipeline(categories, category_dfs_encoded, test_df, num_epoch = 10)

# alldf = []
# for i in categories:
#     df = pd.read_csv(f'/kaggle/input/output/{i}_predictions (1).csv')
#     alldf.append(df)


# alldf = []
# for i in categories:
#     if i == 'Women Tops & Tunics':
#         i = 'Women Tops and Tunics'
#     df = pd.read_csv(f'/kaggle/input/ouputt/{i}_predictions (1).csv')
#     alldf.append(df)

import torch
from transformers import ViTForImageClassification, ViTFeatureExtractor
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import pandas as pd
import numpy as np
import os
import torch.optim as optim
import torch.nn as nn

# Load the ViT feature extractor
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# Set device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Custom dataset class for ViT
class ProductDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]
        image_path = row['image_path']
        image = Image.open(image_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        labels = row.iloc[3:].values.astype(float)  # Starting from 4th column onwards (attributes)
        return image, torch.tensor(labels, dtype=torch.float32)

# Apply ViT feature extractor transformations to the dataset
def vit_transform(image):
    return feature_extractor(images=image, return_tensors="pt").pixel_values[0]

# Prepare dataset
def prepare_dataset(df):
    return ProductDataset(df, transform=vit_transform)

# Initialize the ViT model for multi-label classification
def create_vit_model(num_labels):
    model = ViTForImageClassification.from_pretrained(
        'google/vit-base-patch16-224-in21k',
        num_labels=num_labels
    )
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.3),
        nn.Linear(model.config.hidden_size, num_labels)
    )
    return model

# Training function with early stopping
def train_vit_model_with_early_stopping(model, train_loader, val_loader, num_epochs=10, patience=3):
    model = model.to(device)
    criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)

    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images).logits
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_loss = running_loss / len(train_loader)

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images).logits
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        val_loss /= len(val_loader)

        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        # Early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save the best model
            torch.save(model.state_dict(), 'best_model.pth')
            print("Validation loss improved, saving model.")
        else:
            patience_counter += 1
            print(f"No improvement in validation loss for {patience_counter} epoch(s).")

        # Check if we should stop early
        if patience_counter >= patience:
            print("Early stopping triggered. Training stopped.")
            break

    # Load the best model
    model.load_state_dict(torch.load('best_model.pth'))
    return model

# Generalized function for creating, training, and early-stopping the ViT model
def create_and_train_vit_model_for_category(df_encoded, val_df, category_name, num_epochs=10, patience=3):
    num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'
    vit_model = create_vit_model(num_labels)

    # Prepare datasets and DataLoaders
    train_dataset = prepare_dataset(df_encoded)
    val_dataset = prepare_dataset(val_df)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # Train model with early stopping
    trained_model = train_vit_model_with_early_stopping(vit_model, train_loader, val_loader, num_epochs, patience)

    # Save the trained model
    model_filename = f'vit_model_{category_name}.pth'
    torch.save(trained_model.state_dict(), model_filename)
    print(f"Model for {category_name} saved as {model_filename}")

    return trained_model

# Testing function for ViT model
def test_vit_model(df_test, category_name, num_labels):
    test_dataset = prepare_dataset(df_test)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Load the trained model
    model = create_vit_model(num_labels)
    model.load_state_dict(torch.load(f'vit_model_{category_name}.pth'))
    model = model.to(device)
    model.eval()

    predictions = []
    with torch.no_grad():
        for images, _ in test_loader:
            images = images.to(device)
            outputs = model(images).logits
            preds = torch.sigmoid(outputs)
            predictions.append(preds.cpu().numpy())

    return np.concatenate(predictions, axis=0)

# Helper functions for decoding predictions and preparing submissions would go here
# Similar to your original setup, we assume a function called decode_predictions exists

# Pipeline function for training, testing, and submission preparation
def pipeline(categories, category_dfs_encoded, category_dfs_val, test_df, num_epoch=10, patience=3):
    for category in categories:
        print(f"\nProcessing category: {category}")

        # Load data for the category
        df_encoded = category_dfs_encoded[category]
        val_df = category_dfs_val[category]
        num_labels = df_encoded.shape[1] - 3  # Adjust for other columns

        # Train with early stopping
        trained_model = create_and_train_vit_model_for_category(df_encoded, val_df, category, num_epoch, patience)

        # Prepare and save test predictions
        df_test_category = test_df[test_df['Category'] == category].copy()
        df_test_category['image_path'] = df_test_category['id'].apply(lambda x: f"/path/to/test_images/{x}.jpg")
        predictions = test_vit_model(df_test_category, category, num_labels)

        # Decode and save predictions for submission
        decoded_predictions = decode_predictions(predictions, encoders[category])
        prepare_submission(df_test_category, decoded_predictions, category, num_labels)

# Run the pipeline with your categories and data
# Replace category_dfs_encoded, category_dfs_val, test_df, and encoders with actual data

categories = ['Sarees', 'Kurtis', 'Women Tshirts', 'Women Tops & Tunics', 'Men Tshirts']
pipeline(categories, category_dfs_encoded, category_dfs_encoded, test_df, num_epoch=20, patience=4)

alldf = []
for i in categories:
    df = pd.read_csv(f'/kaggle/working/{i}_predictions.csv')
    alldf.append(df)

len(alldf)

df_final = pd.concat(alldf, axis = 0)
df_final = df_final.sort_values(by = 'id')
df_final.reset_index(drop= True, inplace = True)

df_final = pd.concat([df_final, pd.read_csv('/kaggle/input/visual-taxonomy/test.csv')['Category']], axis = 1)

df_final['len'] = df_final['Category'].map(category_attributes)
df_final = df_final[sample.columns]
df_final.fillna('extra', inplace = True)

df_final.to_csv('meesho_submission29_autoep_3na_knn25.csv', index = False)

df_final

pd.read_csv('/kaggle/working/meesho_submission26_10ep_4na_knn17.csv')











# import torch
# from transformers import ViTForImageClassification, ViTFeatureExtractor
# from torch.utils.data import DataLoader
# from PIL import Image
# import pandas as pd
# import numpy as np
# import os
# import torch.optim as optim
# import torch.nn as nn

# # Load the feature extractor
# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# # Set device (GPU or CPU)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # Custom dataset class for ViT
# class ProductDataset(torch.utils.data.Dataset):
#     def __init__(self, dataframe, transform=None):
#         self.dataframe = dataframe
#         self.transform = transform

#     def __len__(self):
#         return len(self.dataframe)

#     def __getitem__(self, idx):
#         row = self.dataframe.iloc[idx]
#         image_path = row['image_path']
#         image = Image.open(image_path).convert('RGB')
#         if self.transform:
#             image = self.transform(image)
#         labels = row.iloc[3:].values.astype(float)  # Starting from 4th column onwards (attributes)
#         return image, torch.tensor(labels, dtype=torch.float32)

# # Apply ViT feature extractor transformations to the dataset
# def vit_transform(image):
#     return feature_extractor(images=image, return_tensors="pt").pixel_values[0]

# # Prepare dataset
# def prepare_dataset(df):
#     return ProductDataset(df, transform=vit_transform)

# # Initialize the ViT model for multi-label classification
# def create_vit_model(num_labels):
#     model = ViTForImageClassification.from_pretrained(
#         'google/vit-base-patch16-224-in21k',
#         num_labels=num_labels
#     )
#     model.classifier = nn.Sequential(
#         nn.Dropout(p=0.3),
#         nn.Linear(model.config.hidden_size, num_labels)
#     )
#     return model

# def train_vit_model(model, dataloader, num_epochs=1):
#     model = model.to(device)
#     criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification
#     optimizer = optim.AdamW(model.parameters(), lr=1e-4)

#     model.train()
#     for epoch in range(num_epochs):
#         running_loss = 0.0
#         for images, labels in dataloader:
#             images, labels = images.to(device), labels.to(device)
#             optimizer.zero_grad()
#             outputs = model(images).logits  # Forward pass
#             loss = criterion(outputs, labels)
#             loss.backward()  # Backpropagation
#             optimizer.step()  # Update weights
#             running_loss += loss.item()
#         print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}")

#     return model

# # Generalized function for creating and training the ViT model for any category
# def create_and_train_vit_model_for_category(df_encoded, category_name, num_epochs=3):
#     # Calculate the number of labels based on the encoded dataframe
#     num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'

#     # Create the ViT model
#     vit_model = create_vit_model(num_labels)

#     # Prepare dataset and DataLoader
#     train_dataset = prepare_dataset(df_encoded)
#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

#     # Train the model
#     trained_model = train_vit_model(vit_model, train_loader, num_epochs)

#     # Save the trained model
#     model_filename = f'vit_model_{category_name}.pth'
#     torch.save(trained_model.state_dict(), model_filename)
#     print(f"Model for {category_name} saved as {model_filename}")

#     return trained_model

# # Function to load and test the model for a given category
# def test_vit_model(df_test, category_name, num_labels):
#     # Prepare test dataset
#     test_dataset = prepare_dataset(df_test)
#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

#     # Load the trained model
#     model = create_vit_model(num_labels)
#     model.load_state_dict(torch.load(f'vit_model_{category_name}.pth'))
#     model = model.to(device)
#     model.eval()

#     # Make predictions
#     predictions = []
#     with torch.no_grad():
#         for images, ids in test_loader:
#             images = images.to(device)
#             outputs = model(images).logits
#             preds = torch.sigmoid(outputs)
#             predictions.append(preds.cpu().numpy())

#     return np.concatenate(predictions, axis=0)

# # Decode predictions using the OneHotEncoder dynamically based on the category and number of attributes
# def decode_predictions(predictions, encoder):
#     decoded_predictions = []

#     for pred in predictions:
#         decoded = []
#         val = 0
#         for i in range(len(encoder.categories_)):
#             # Get the probabilities for the current attribute (i-th group of one-hot encoded columns)
#             attr_probs = pred[val : val + len(encoder.categories_[i])]
#             val = val + len(encoder.categories_[i])

#             # Ensure that we pick the category with the highest probability, even if all are below the threshold
#             max_index = np.argmax(attr_probs)  # Find the index of the highest probability

#             decoded.append(encoder.categories_[i][max_index])  # Use the encoder to map back the category

#         decoded_predictions.append(decoded)

#     return np.array(decoded_predictions)

# def prepare_submission(df_test, decoded_attributes, category_name, num_attributes):
#     # Prepare the submission data
#     submission_data = {
#         'id': df_test['id'].values  # Include the 'id' column
#     }

#     # Get the actual number of decoded attributes from the shape of decoded_attributes
#     actual_num_attributes = decoded_attributes.shape[1]

#     # Dynamically add attribute columns to the dictionary based on the actual number of decoded attributes
#     for i in range(1, actual_num_attributes + 1):
#         submission_data[f'attr_{i}'] = decoded_attributes[:, i - 1]  # Adjust for zero-indexing in arrays

#     # Convert the dictionary to a DataFrame
#     submission_df = pd.DataFrame(submission_data)

#     # Save the submission DataFrame to a CSV file
#     submission_filename = f"{category_name}_predictions.csv"
#     submission_df.to_csv(submission_filename, index=False)
#     print(f"Submission file saved: {submission_filename}")


# # Generalized function to handle decoding and submission for any category
# def decode_and_save_submission(predictions, df_test, encoder, category_name, num_attributes):
#     # Convert the predictions array
#     pred_array = predictions

#     # Decode the predictions using the stored OneHotEncoder
#     decoded_attributes = decode_predictions(pred_array, encoder)

#     # Prepare the submission DataFrame
#     submission_df = prepare_submission(df_test, decoded_attributes, num_attributes)

#     # Save the submission DataFrame to CSV
#     submission_file = f'{category_name}_predictions.csv'
#     submission_df.to_csv(submission_file, index=False)

#     print(f"Submission file for {category_name} saved as {submission_file}")

#     return submission_df

# # Function to save the trained model for a specific category
# def save_model(trained_model, category_name):
#     model_filename = f'vit_model_{category_name}.pth'
#     torch.save(trained_model.state_dict(), model_filename)
#     print(f"Model for {category_name} saved as {model_filename}")

# categories = ['Sarees', 'Kurtis', 'Women Tshirts', 'Women Tops & Tunics', 'Men Tshirts']

# def pipeline(categories, category_dfs_encoded, test_df , num_epoch = 3):
#     # Loop over each category provided in the input
#     for category in categories:
#         print(f"\nProcessing category: {category}")

#         # 1. Prepare the dataset and dataloader for the category
#         df_encoded = category_dfs_encoded[category]  # Get the encoded dataframe for the category

#         # 2. Create and train the model for this category
#         num_labels = df_encoded.shape[1] - 3  # Subtract 'id', 'Category', 'image_path'
#         trained_model = create_and_train_vit_model_for_category(df_encoded ,category,num_epoch)

#         # 3. Save the trained model
#         save_model(trained_model, category)

#         # 4. Prepare the test data for this category
#         df_test_category = test_df[test_df['Category'] == category].copy()
#         df_test_category['image_path'] = df_test_category['id'].apply(lambda x: f"/kaggle/input/visual-taxonomy/test_images/{x}.jpg")
#         test_dataset = prepare_dataset(df_test_category)  # Prepare the test dataset
#         test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

#         # 5. Test the model on the test data
#         predictions = test_vit_model(df_test_category, category, num_labels)  # Get predictions
#         print(pd.DataFrame(predictions))

#         # 6. Decode the predictions using the stored OneHotEncoder
#         pred_array = np.concatenate(predictions, axis=0)  # Concatenate along rows
#         pred_array = pred_array.reshape(-1, num_labels)  # Reshape to (samples, num_labels)

#         decoded_predictions = decode_predictions(pred_array, encoders[category])
#         print(pd.DataFrame(decoded_predictions))


#         # 7. Prepare and save the submission for this category
#         prepare_submission(df_test_category, decoded_predictions, category, num_labels)